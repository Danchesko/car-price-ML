{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Cleaning Data</h1></center>\n",
    "***\n",
    "\n",
    "Missing data can be of different types:\n",
    "\n",
    "* **Missing Completely at Random (MCAR)** - the propensity for a data point to be missing is completely random.\n",
    "* **Missing at Random (MAR)** - means  the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.\n",
    "* **Missing Not at Random (MNAR)** - when the missing values on a variable are related to the values of that variable itself, even after controlling for other variable\n",
    "\n",
    "In our dataset we don't have any Missing Completely at Random variables, the color of the car could be MCAR value, but after our Exploratory Analysis we saw that some of the colors are important features of the car. MCAR variables would be the easiest ones to impute in our dataset, MCAR variables are not connected with any other variable in dataset, so I just could impute mean, median or mode values.\n",
    "\n",
    "Most of the variables are Missing at Random, we can impute them via Linear Regression impute, KNN impute, multiple imputation or Maximum likelihood imputation, this values can be predicted by other values. And as we see, we have some feature that is Missing Not at random, it is the worst case scenario, but luckily we can impute this variable too. So, this variable is \"Пробег\".\n",
    "I will show in data itself and plot it to ensure that, this feature, indeed, is MNAR. At first, I will import everything I need. First we read the file, than drop raws with missing target values, because we will not need them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from car_price_prediction.utils import dataset_manager, df_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_manager.get_cleaned_outliers_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what I do right away is, I create a new dataset, where all the samples have NaN in \"Пробег\" column. And from old and new dataset, I take the year column, in order to get number of cars with a certain year where \"Пробег\" is NaN and total number of cars. And next, I create a dataset, where first columns is quantity total number of cars each year and second column is quantity of Nan values in \"Пробег\" column in each year, and third column is percentage, of how many cars from total amount have blank information in \"Пробег\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulldataset= data[~data[\"Пробег\"].notnull()][\"Год выпуска\"]\n",
    "totaldataset = data[\"Год выпуска\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All cars</th>\n",
       "      <th>Cars with NaN</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970.0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980.0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982.0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983.0</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984.0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985.0</th>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>89.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986.0</th>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987.0</th>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988.0</th>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>80.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989.0</th>\n",
       "      <td>71</td>\n",
       "      <td>51</td>\n",
       "      <td>71.830986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990.0</th>\n",
       "      <td>69</td>\n",
       "      <td>52</td>\n",
       "      <td>75.362319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991.0</th>\n",
       "      <td>96</td>\n",
       "      <td>68</td>\n",
       "      <td>70.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992.0</th>\n",
       "      <td>137</td>\n",
       "      <td>97</td>\n",
       "      <td>70.802920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993.0</th>\n",
       "      <td>140</td>\n",
       "      <td>103</td>\n",
       "      <td>73.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994.0</th>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995.0</th>\n",
       "      <td>204</td>\n",
       "      <td>136</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996.0</th>\n",
       "      <td>159</td>\n",
       "      <td>109</td>\n",
       "      <td>68.553459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997.0</th>\n",
       "      <td>160</td>\n",
       "      <td>112</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998.0</th>\n",
       "      <td>232</td>\n",
       "      <td>160</td>\n",
       "      <td>68.965517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999.0</th>\n",
       "      <td>273</td>\n",
       "      <td>168</td>\n",
       "      <td>61.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000.0</th>\n",
       "      <td>529</td>\n",
       "      <td>306</td>\n",
       "      <td>57.844991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001.0</th>\n",
       "      <td>737</td>\n",
       "      <td>472</td>\n",
       "      <td>64.043419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002.0</th>\n",
       "      <td>1071</td>\n",
       "      <td>614</td>\n",
       "      <td>57.329599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003.0</th>\n",
       "      <td>1412</td>\n",
       "      <td>761</td>\n",
       "      <td>53.895184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004.0</th>\n",
       "      <td>945</td>\n",
       "      <td>510</td>\n",
       "      <td>53.968254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005.0</th>\n",
       "      <td>664</td>\n",
       "      <td>325</td>\n",
       "      <td>48.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006.0</th>\n",
       "      <td>428</td>\n",
       "      <td>205</td>\n",
       "      <td>47.897196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007.0</th>\n",
       "      <td>407</td>\n",
       "      <td>187</td>\n",
       "      <td>45.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008.0</th>\n",
       "      <td>577</td>\n",
       "      <td>139</td>\n",
       "      <td>24.090121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009.0</th>\n",
       "      <td>249</td>\n",
       "      <td>55</td>\n",
       "      <td>22.088353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010.0</th>\n",
       "      <td>446</td>\n",
       "      <td>55</td>\n",
       "      <td>12.331839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011.0</th>\n",
       "      <td>598</td>\n",
       "      <td>38</td>\n",
       "      <td>6.354515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012.0</th>\n",
       "      <td>758</td>\n",
       "      <td>26</td>\n",
       "      <td>3.430079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013.0</th>\n",
       "      <td>644</td>\n",
       "      <td>22</td>\n",
       "      <td>3.416149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014.0</th>\n",
       "      <td>520</td>\n",
       "      <td>20</td>\n",
       "      <td>3.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015.0</th>\n",
       "      <td>476</td>\n",
       "      <td>5</td>\n",
       "      <td>1.050420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016.0</th>\n",
       "      <td>435</td>\n",
       "      <td>11</td>\n",
       "      <td>2.528736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017.0</th>\n",
       "      <td>408</td>\n",
       "      <td>5</td>\n",
       "      <td>1.225490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018.0</th>\n",
       "      <td>222</td>\n",
       "      <td>9</td>\n",
       "      <td>4.054054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        All cars  Cars with NaN  percentage\n",
       "1961.0         1              1  100.000000\n",
       "1964.0         1              1  100.000000\n",
       "1966.0         1              1  100.000000\n",
       "1968.0         1              1  100.000000\n",
       "1970.0         2              2  100.000000\n",
       "1971.0         1              1  100.000000\n",
       "1976.0         1              1  100.000000\n",
       "1980.0         4              4  100.000000\n",
       "1981.0         1              1  100.000000\n",
       "1982.0         3              2   66.666667\n",
       "1983.0         7              6   85.714286\n",
       "1984.0         3              3  100.000000\n",
       "1985.0        19             17   89.473684\n",
       "1986.0        22             16   72.727273\n",
       "1987.0        36             27   75.000000\n",
       "1988.0        57             46   80.701754\n",
       "1989.0        71             51   71.830986\n",
       "1990.0        69             52   75.362319\n",
       "1991.0        96             68   70.833333\n",
       "1992.0       137             97   70.802920\n",
       "1993.0       140            103   73.571429\n",
       "1994.0       160            110   68.750000\n",
       "1995.0       204            136   66.666667\n",
       "1996.0       159            109   68.553459\n",
       "1997.0       160            112   70.000000\n",
       "1998.0       232            160   68.965517\n",
       "1999.0       273            168   61.538462\n",
       "2000.0       529            306   57.844991\n",
       "2001.0       737            472   64.043419\n",
       "2002.0      1071            614   57.329599\n",
       "2003.0      1412            761   53.895184\n",
       "2004.0       945            510   53.968254\n",
       "2005.0       664            325   48.945783\n",
       "2006.0       428            205   47.897196\n",
       "2007.0       407            187   45.945946\n",
       "2008.0       577            139   24.090121\n",
       "2009.0       249             55   22.088353\n",
       "2010.0       446             55   12.331839\n",
       "2011.0       598             38    6.354515\n",
       "2012.0       758             26    3.430079\n",
       "2013.0       644             22    3.416149\n",
       "2014.0       520             20    3.846154\n",
       "2015.0       476              5    1.050420\n",
       "2016.0       435             11    2.528736\n",
       "2017.0       408              5    1.225490\n",
       "2018.0       222              9    4.054054"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nullval = nulldataset.value_counts()\n",
    "totalval = totaldataset.value_counts()\n",
    "\n",
    "percentage = 100.0/totalval*nullval\n",
    "\n",
    "df =  pd.DataFrame({\"All cars\":totalval,\"Cars with NaN\":nullval,\"percentage\":percentage})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see yourself that people tend to not specify the mileage of the car, when the car is older. I can plot it. X is the percentage of cars with unindicated mileage as opposed to year of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Percentage')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb1d8650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df.index,percentage,marker=\"o\",linewidths=2)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the problem, I can not impute any of the missing values in the dataset by median, mean or mode, because it will definetely create bias in the model. So, I need to use more sophisticated ways of imputing, but unfortunately in scikit there is no convinient library for doing so, there is a library called \"fancyimputer\", but since pipinstall shows a lot of dependencies when downloading the library, I couldn't import it, because it was dependent from tensorflow library, which I couldn't download, cause it was incompetable with my system.  \n",
    "So, the solution will be to manually impute the values. Which I will do in notebook-helper function folder **imputting.py**. \n",
    "In this module, I did next manipulations:\n",
    "\n",
    "* Dropped rows with missing target values, because anyway they wouldn't really help us\n",
    "* Dropped row \"Тип кузова\" because there was not a lot of missing rows with Nan in this row, so it didn't create any bias or it didn't reduce variance of model\n",
    "* Drop year row because of the same reason, but also year is one of the most needed features, so I didn't risk to create bias, imputing this feature\n",
    "* Imputed color feature, by making all of the unspecified samples - another class. It helps to reduce bias\n",
    "* I did the same with fuel feature, because I was not sure if the unspecified fuel type was just unspecified, or because there were some different type of cars(electro for example), which users of webpage couldn't specify.\n",
    "* I imputed wheel type by KNN imputation, I chose most important features for predicting the type of wheel, those features were[\"Цена\",\"Тип кузова\"], and I predicted the side of wheel in the car\n",
    "* I as well imputed - [\"Привод\"],[\"КПП\"],[\"Объём\"],[\"Мощность\"],['Пробег'] features by predicting their values by KNN machine learning algorithm, using most linearly correlated features of theirs.\n",
    "\n",
    "So now, you can see number of missing data from not imputed data, and you can see number of missing data from imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Год выпуска     185\n",
       "КПП             287\n",
       "Марка             0\n",
       "Мощность       2961\n",
       "Объём           772\n",
       "Привод         1015\n",
       "Пробег         5187\n",
       "Руль            693\n",
       "Тип кузова       48\n",
       "Топливо         277\n",
       "Цвет           1047\n",
       "Цена              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helper_functions import data_impute\n",
    "import warnings\n",
    "#ignoring warnings, erroneously raised by imported library\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    imputed_data = data_impute.get_imputed_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Год выпуска    0\n",
       "КПП            0\n",
       "Марка          0\n",
       "Мощность       0\n",
       "Объём          0\n",
       "Привод         0\n",
       "Пробег         0\n",
       "Руль           0\n",
       "Тип кузова     0\n",
       "Топливо        0\n",
       "Цвет           0\n",
       "Цена           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see, that no missing data left, that makes for us possible using machine learning algorithms. Next thing I will do, I will create a dataset, dropping every row with missing data, from old raw data. Thus, I will see, were my hypothesis right, about biasing my dataset, and we will see which model will be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will see differences between imputed and dropped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Год выпуска    0\n",
       "КПП            0\n",
       "Марка          0\n",
       "Мощность       0\n",
       "Объём          0\n",
       "Привод         0\n",
       "Пробег         0\n",
       "Руль           0\n",
       "Тип кузова     0\n",
       "Топливо        0\n",
       "Цвет           0\n",
       "Цена           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped_data = data.copy()\n",
    "dropped_data = data.dropna()\n",
    "dropped_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both of the datasets have no missing values. But the shape as you can see below, is radically different. Dropped dataset two times smaller than imputed dataset. But sometimes it doesn't help, so I hope imputed data will make better in predictions, because imputted I nearly for 6 hours, and dropped values in 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13572, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6958, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I tried to create maximum unbiased dataset, in that dataset I imputed just Missing Not At Random feature. And others I imputed a little differently:\n",
    "* Some columns with missing categorical feauteres, I made as a one different class\n",
    "* Other columns, when there was no meaning in making a different class for missing values, I just dropped\n",
    "* I imputed just Not Missing at Random feature\n",
    "* And other missing continious features, I dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11165, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max_unbiased = dataset_manager.get_processed_dataset()\n",
    "data_max_unbiased.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next notebook, I will try to choose the dataset, that will have the smallest bias in it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
