{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Cleaning Data</h1></center><br>\n",
    "Missing data can be of different types: <br>\n",
    "<ul>\n",
    "    <li><b>Missing Completely at Random (MCAR)</b> - the propensity for a data point to be missing is completely random.<br></li>\n",
    "<li><b>Missing at Random (MAR)</b> - means  the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.<br></li>\n",
    "<li><b>Missing Not at Random (MNAR)</b> - when the missing values on a variable are related to the values of that variable itself, even after controlling for other variable</li></ul><br>\n",
    "In our dataset we don't have any Missing Completely at Random variables, the color of the car could be MCAR value, but after our Exploratory Analysis we saw that some of the colors are important features of the car. MCAR variables would be the easiest ones to impute in our dataset, MCAR variables are not connected with any other variable in dataset, so I just could impute mean, median or mode values.\n",
    "<br>Most of the variables are Missing at Random, we can impute them via Linear Regression impute, KNN impute, multiple imputation or Maximum likelihood imputation, this values can be predicted by other values. And as we see, we have some feature that is Missing Not at random, it is the worst case scenario, but luckily we can impute this variable too. So, this variable is \"Пробег\".\n",
    "I will show in data itself and plot it to ensure that, this feature, indeed, is MNAR. At first, I will import everything I need. First we read the file, than drop raws with missing target values, because we will not need them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data_extraction import get_X_and_y, load_or_save_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_or_save_dataset.get_cleaned_outliers_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what I do right away is, I create a new dataset, where all the samples have NaN in \"Пробег\" column. And from old and new dataset, I take the year column, in order to get number of cars with a certain year where \"Пробег\" is NaN and total number of cars. And next, I create a dataset, where first columns is quantity total number of cars each year and second column is quantity of Nan values in \"Пробег\" column in each year, and third column is percentage, of how many cars from total amount have blank information in \"Пробег\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulldataset= data[~data[\"Пробег\"].notnull()][\"Год выпуска\"]\n",
    "totaldataset = data[\"Год выпуска\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All cars</th>\n",
       "      <th>Cars with NaN</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970.0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972.0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982.0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984.0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985.0</th>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986.0</th>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987.0</th>\n",
       "      <td>12</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988.0</th>\n",
       "      <td>27</td>\n",
       "      <td>18.0</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989.0</th>\n",
       "      <td>36</td>\n",
       "      <td>20.0</td>\n",
       "      <td>55.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990.0</th>\n",
       "      <td>33</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991.0</th>\n",
       "      <td>47</td>\n",
       "      <td>27.0</td>\n",
       "      <td>57.446809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992.0</th>\n",
       "      <td>63</td>\n",
       "      <td>41.0</td>\n",
       "      <td>65.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993.0</th>\n",
       "      <td>70</td>\n",
       "      <td>47.0</td>\n",
       "      <td>67.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994.0</th>\n",
       "      <td>67</td>\n",
       "      <td>36.0</td>\n",
       "      <td>53.731343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995.0</th>\n",
       "      <td>112</td>\n",
       "      <td>70.0</td>\n",
       "      <td>62.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996.0</th>\n",
       "      <td>83</td>\n",
       "      <td>47.0</td>\n",
       "      <td>56.626506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997.0</th>\n",
       "      <td>85</td>\n",
       "      <td>43.0</td>\n",
       "      <td>50.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998.0</th>\n",
       "      <td>139</td>\n",
       "      <td>80.0</td>\n",
       "      <td>57.553957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999.0</th>\n",
       "      <td>163</td>\n",
       "      <td>73.0</td>\n",
       "      <td>44.785276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000.0</th>\n",
       "      <td>306</td>\n",
       "      <td>144.0</td>\n",
       "      <td>47.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001.0</th>\n",
       "      <td>408</td>\n",
       "      <td>220.0</td>\n",
       "      <td>53.921569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002.0</th>\n",
       "      <td>611</td>\n",
       "      <td>284.0</td>\n",
       "      <td>46.481178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003.0</th>\n",
       "      <td>812</td>\n",
       "      <td>328.0</td>\n",
       "      <td>40.394089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004.0</th>\n",
       "      <td>528</td>\n",
       "      <td>206.0</td>\n",
       "      <td>39.015152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005.0</th>\n",
       "      <td>406</td>\n",
       "      <td>161.0</td>\n",
       "      <td>39.655172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006.0</th>\n",
       "      <td>218</td>\n",
       "      <td>69.0</td>\n",
       "      <td>31.651376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007.0</th>\n",
       "      <td>233</td>\n",
       "      <td>85.0</td>\n",
       "      <td>36.480687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008.0</th>\n",
       "      <td>286</td>\n",
       "      <td>56.0</td>\n",
       "      <td>19.580420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009.0</th>\n",
       "      <td>130</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010.0</th>\n",
       "      <td>223</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011.0</th>\n",
       "      <td>240</td>\n",
       "      <td>27.0</td>\n",
       "      <td>11.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012.0</th>\n",
       "      <td>330</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013.0</th>\n",
       "      <td>256</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014.0</th>\n",
       "      <td>224</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.910714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015.0</th>\n",
       "      <td>250</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016.0</th>\n",
       "      <td>253</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.976285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017.0</th>\n",
       "      <td>251</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.195219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018.0</th>\n",
       "      <td>135</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.962963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        All cars  Cars with NaN  percentage\n",
       "1961.0         1            1.0  100.000000\n",
       "1964.0         1            1.0  100.000000\n",
       "1968.0         1            1.0  100.000000\n",
       "1970.0         2            2.0  100.000000\n",
       "1971.0         1            1.0  100.000000\n",
       "1972.0         1            NaN         NaN\n",
       "1976.0         1            1.0  100.000000\n",
       "1982.0         2            2.0  100.000000\n",
       "1984.0         1            1.0  100.000000\n",
       "1985.0         8            7.0   87.500000\n",
       "1986.0         8            6.0   75.000000\n",
       "1987.0        12            9.0   75.000000\n",
       "1988.0        27           18.0   66.666667\n",
       "1989.0        36           20.0   55.555556\n",
       "1990.0        33           20.0   60.606061\n",
       "1991.0        47           27.0   57.446809\n",
       "1992.0        63           41.0   65.079365\n",
       "1993.0        70           47.0   67.142857\n",
       "1994.0        67           36.0   53.731343\n",
       "1995.0       112           70.0   62.500000\n",
       "1996.0        83           47.0   56.626506\n",
       "1997.0        85           43.0   50.588235\n",
       "1998.0       139           80.0   57.553957\n",
       "1999.0       163           73.0   44.785276\n",
       "2000.0       306          144.0   47.058824\n",
       "2001.0       408          220.0   53.921569\n",
       "2002.0       611          284.0   46.481178\n",
       "2003.0       812          328.0   40.394089\n",
       "2004.0       528          206.0   39.015152\n",
       "2005.0       406          161.0   39.655172\n",
       "2006.0       218           69.0   31.651376\n",
       "2007.0       233           85.0   36.480687\n",
       "2008.0       286           56.0   19.580420\n",
       "2009.0       130           23.0   17.692308\n",
       "2010.0       223           29.0   13.004484\n",
       "2011.0       240           27.0   11.250000\n",
       "2012.0       330           16.0    4.848485\n",
       "2013.0       256           17.0    6.640625\n",
       "2014.0       224           11.0    4.910714\n",
       "2015.0       250            3.0    1.200000\n",
       "2016.0       253            5.0    1.976285\n",
       "2017.0       251            3.0    1.195219\n",
       "2018.0       135            4.0    2.962963"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nullval = nulldataset.value_counts()\n",
    "totalval = totaldataset.value_counts()\n",
    "\n",
    "percentage = 100.0/totalval*nullval\n",
    "\n",
    "df =  pd.DataFrame({\"All cars\":totalval,\"Cars with NaN\":nullval,\"percentage\":percentage})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see yourself that people tend to not specify the mileage of the car, when the car is older. I can plot it. X is the percentage of cars with unindicated mileage as opposed to year of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Percentage')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa9f9030>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df.index,percentage,marker=\"o\",linewidths=2)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the problem, I can not impute any of the missing values in the dataset by median, mean or mode, because it will definetely create bias in the model. So, I need to use more sophisticated ways of imputing, but unfortunately in scikit there is no convinient library for doing so, there is a library called \"fancyimputer\", but since pipinstall shows a lot of dependencies when downloading the library, I couldn't import it, because it was dependent from tensorflow library, which I couldn't download, cause it was incompetable with my system<br>\n",
    "So, the solution will be to manually impute the values. Which I will do in clean_data.py. \n",
    "In the clean_data.py, I did next manipulations:\n",
    "<ul>\n",
    "     <li>Dropped rows with missing target values, because anyway they wouldn't really help us</li>\n",
    "     <li>Dropped row \"Тип кузова\" because there was not a lot of missing rows with Nan in this row, so it didn't create any bias or it didn't reduce variance of model</li>\n",
    "     <li>Drop year row because of the same reason, but also year is one of the most needed features, so I didn't risk to create bias, imputing this feature</li>\n",
    "     <li>Imputed color feature, by making all of the unspecified samples - another class. It helps to reduce bias</li>\n",
    "     <li>I did the same with fuel feature, because I was not sure if the unspecified fuel type was just unspecified, or because there were some different type of cars(electro for example), which users of webpage couldn't specify. </li>\n",
    "     <li>I imputed wheel type by KNN imputation, I chose most important features for predicting the type of wheel, those features were[\"Цена\",\"Тип кузова\"], and I predicted the side of wheel in the car</li>\n",
    "     <li>I as well imputed - [\"Привод\"],[\"КПП\"],[\"Объём\"],[\"Мощность\"],['Пробег'] features by predicting their values by KNN machine learning algorithm, using most linearly correlated features of theirs.</li>\n",
    "    <li>And, lastly, I wrote all of imputed data to new .xslx file.\n",
    "</ul>\n",
    "So now, you can see number of missing data from not imputed data, and you can see number of missing data from imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Год выпуска      90\n",
       "КПП             159\n",
       "Мощность       1615\n",
       "Объём           407\n",
       "Привод          587\n",
       "Пробег         2308\n",
       "Руль            371\n",
       "Тип кузова       19\n",
       "Топливо         147\n",
       "Цвет            563\n",
       "Цена              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Год выпуска    0\n",
       "КПП            0\n",
       "Мощность       0\n",
       "Объём          0\n",
       "Привод         0\n",
       "Пробег         0\n",
       "Руль           0\n",
       "Тип кузова     0\n",
       "Топливо        0\n",
       "Цвет           0\n",
       "Цена           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_or_save_dataset.get_imputed_nans_dataset()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see, that no missing data left, that makes for us possible using machine learning algorithms. Next thing I will do, I will create a dataset, dropping every row with missing data, from old raw data. Thus, I will see, were my hypothesis right, about biasing my datase, and we will see which model will be more accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will see differences between imputed and dropped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Год выпуска    0\n",
       "КПП            0\n",
       "Мощность       0\n",
       "Объём          0\n",
       "Привод         0\n",
       "Пробег         0\n",
       "Руль           0\n",
       "Тип кузова     0\n",
       "Топливо        0\n",
       "Цвет           0\n",
       "Цена           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dropped = load_or_save_dataset.get_dropped_nans_dataset()\n",
    "data_dropped.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both of the datasets have no missing values. But the shape as you can see below, is radically different. Dropped dataset two times smaller than imputed dataset. But sometimes it doesn't help, so I hope imputed data will make better in predictions, because imputted I nearly for 6 hours, and dropped values in 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7115, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3504, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I tried to create maximum unbiased dataset, in that dataset I imputed just Missing Not At Random feature. And others I imputed a little differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max_unbiased = load_or_save_dataset.get_processed_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6024, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max_unbiased.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing, I will perform is choose Dataset to perform modelling on, and after I will standartize, and make categorical features - numerical, so it will be easier for our algorithms to find optimal solution. In python there is no way to use categorical features without modifications in predictions, there are other ways in other languages, and I hope soon, those methods will be implemented in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
