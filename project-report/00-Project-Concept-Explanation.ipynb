{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"display:inline;\"> Project-Based Internship </h1></center>\n",
    "<h3 align = \"right\"> by Daniiar Berdikulov</h3>\n",
    "<br>\n",
    "The idea of the project was to create a program, that works on machine learning algorithm: it takes data of cars' features and the price of the car and learns how to predict a car's price with given features.\n",
    "Tools I am going to use: \n",
    "<ul>\n",
    "    <li>Python programming language - most popular programming language for data science by now</li>\n",
    "    <li>Anaconda - distribution of python for data science</li>\n",
    "    <li>Jupyter Notebooks - for presenting my project, and showing how it works</li>\n",
    "    <li>Spyder - IDE for data science</li>\n",
    " \n",
    "</ul>\n",
    "\n",
    "After surfing Internet I couldn't find any open source data for cars under sale in Bishkek, so I decided to collect the data by myself. The most used webpages like [diesel.kg](diesel.kg) and [mashina.kg](mashina.kg), couldn't be used as a source of information, because these webpages were impossible to scrape due to randomness of creating web link. Luckily, I found a webpage [cars.kg](cars.kg), which had a pattern in creating web links to each of the car's advertisements and had a decent structure of html, so I was able to scrape data from this webpage.\n",
    "<br>\n",
    "<br>\n",
    "You can see scraping algorithm in \"/src/data-loading/page-scraper.py\"<br>\n",
    "I used next libraries when making a \"scraper\":\n",
    "<ul>\n",
    "    <li>urllib - which helped me to get the html code of the page</li>\n",
    "    <li>BeautifulSoup - which is python library for pulling data out of HTML </li>\n",
    "    <li>re - Python regular expressions</li>\n",
    "    <li>pandas - data-analisys library</li>\n",
    "    <li>csv - which helped me to save data in csv file</li>\n",
    "    <li>os - OS module</li>\n",
    "</ul>\n",
    "The iterator was run for 16000 iterations, and from 16000 about 7400 pages existed, which gave me a pretty good dataset for analysis and prediction.\n",
    "<br>\n",
    "<br>\n",
    "<b>Which features of car I used to feed the machine learning algorithm?</b> <br>\n",
    "I used all of the features that webpage could provide me, that were:\n",
    "<ul>\n",
    "    <li>Год выпуска</li>\n",
    "    <li>КПП</li>\n",
    "    <li>Мощность</li>\n",
    "    <li>Объём</li>\n",
    "    <li>Привод</li>\n",
    "    <li>Пробег</li>\n",
    "    <li>Руль</li>\n",
    "    <li>Тип кузова</li>\n",
    "    <li>Топливо</li>\n",
    "    <li>Цвет</li>\n",
    "    <li>Цена</li>\n",
    "</ul>\n",
    "\n",
    "Some of the features may seem not connected with price (like color of the car), and some features may seem redundant - linearly dependable (like power of the engine and its capacity), but in next notebook \"Explaratory Data Analysis\" I will show which of the features are important, which are redundant - by [Statistical analysis of data](https://en.wikipedia.org/wiki/Data_analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
