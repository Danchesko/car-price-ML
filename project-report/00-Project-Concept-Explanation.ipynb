{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"{dispay:inline}\"><center> Project Concept Explanation</center></h1>\n",
    "\n",
    "***\n",
    "\n",
    "<h3 align = \"right\"> by Daniiar Berdikulov</h3>\n",
    "<br>\n",
    "The idea of the project was to create a program that can predict price of the car in Kyrgyz Republic given parameters. The prediction is based on machine learning algorithms.\n",
    "\n",
    "Tools I am going to use: \n",
    "\n",
    "* Python programming language - most popular programming language for data science by now.\n",
    "* Anaconda - distribution of python for data science.\n",
    "* Jupyter Notebooks - for presenting my project, and showing how it works.\n",
    "* Spyder - IDE for data science.\n",
    "* H20 - open source machine learning and artificial intelligence platform.\n",
    "\n",
    "After surfing Internet I couldn't find any open source data for cars under sale in Bishkek, so I decided to collect the data by myself. The most used webpages like [diesel.kg](diesel.kg) and [mashina.kg](mashina.kg), could be used as a source of information for the project, but the problem was that there were different types of users in these sites, and I would have needed to parse both websites. That wouldn't be a problem, if [diesel.kg](diesel.kg) had some form of structure in its advertisements, but due to the lack of this structure, it wasn't possible to parse it. Luckily, I found a website [cars.kg](cars.kg), which was kind of an aggregator that gathered advertisements from different platforms and had a decent structure of html, so all the data I have now is from this webpage.\n",
    "\n",
    "\n",
    "You can see scraping algorithms in \"/src/data-scraping\".  \n",
    "\n",
    "There are two ways to scrape the data from this webpage: asynchronous scraping and synchronous.\n",
    "I wouldn't recommend using asynchronous scraping due to the increase on the server load of the website, but if the semaphore (number of simultaneous requests to the webpage) is set to the low value, it wouldn't be much of a problem. Usually 100 requests does a good job and doesn't overload the server. \n",
    "\n",
    "The iterator was run for 250000 iteration with asynchronous requests, and from all of this iterations more than 85000 pages were retrieved, which gave me a pretty decent dataset.\n",
    "\n",
    "Using asynchronous script - I spent about an hour waiting for the script to finish. If this many webpages were about to be requested, synchronous script would take several days. \n",
    "\n",
    "**Which features I retrieved from every webpage?**  \n",
    "I used all of the logical features that webpage provided me, that were:\n",
    "\n",
    "* Url \n",
    "* Expiration \n",
    "* Year \n",
    "* Publication \n",
    "* Transmission \n",
    "* Brand \n",
    "* Model \n",
    "* Power \n",
    "* Capacity \n",
    "* Drive \n",
    "* Mileage \n",
    "* Wheel \n",
    "* Carcass \n",
    "* Fuel \n",
    "* Photo_Urls \n",
    "* Color \n",
    "* Price\n",
    "\n",
    "Some of the retrieved features were gathered for their use in the mobile application that, on the moment of writing this notebook, is yet to be developed.\n",
    "\n",
    "**Which features of car I used to feed the machine learning algorithm?**  \n",
    "I used features that logically would make an impact to the price, and did some feature engineering for some of them that wouldn't help in their raw format.\n",
    "\n",
    "* Year \n",
    "* Publication \n",
    "* Transmission \n",
    "* Brand \n",
    "* Model \n",
    "* Power \n",
    "* Capacity \n",
    "* Drive \n",
    "* Mileage \n",
    "* Wheel \n",
    "* Carcass \n",
    "* Fuel \n",
    "* Color \n",
    "* Price\n",
    "\n",
    "Some of the features may seem not connected with price (like color of the car), and some features may seem redundant - linearly dependable (like power of the engine and its capacity), but I will show which of the features are important, which are redundant in the subsequent series of notebooks by [Statistical analysis of data](https://en.wikipedia.org/wiki/Data_analysis)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
