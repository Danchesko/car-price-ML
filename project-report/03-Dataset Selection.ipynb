{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Data Selection</h1></center>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will see, which dataset is more biased, or gives more mean squared error with same model. First I need to import libraries I need to check with which dataset my models perform better. To check I will use a technique called cross-validation. I will perform prediction on three machine learning algorithms. And then I import datasets I need.  \n",
    "Also we need our dataset to be shuffled. So far I made three different datasets. They will be shaffled, in script. Third dataset is mix of first and second dataset. Some values are dropped, some are imputed with another categorical values, and \"Пробег\" column is the only column subjected to predictive imputation, the reason I created the third dataset was in hope to maximally reduce the dataset bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import set_jupyter_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from src.car_price_prediction.utils import dataset_manager, df_utils\n",
    "data = dataset_manager.get_cleaned_outliers_dataset()\n",
    "from notebook_helper_functions.data_impute import get_imputed_dataset\n",
    "\n",
    "def make_imputed_dataset(data):\n",
    "    data_for_imp = data.copy()\n",
    "    #erroneous warining inside imported library\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        data_imp = get_imputed_dataset(data_for_imp)\n",
    "    data_for_imp = shuffle(data_for_imp)\n",
    "    return data_imp\n",
    "\n",
    "def get_dropped_dataset(data):\n",
    "    data_for_drop = data.copy()\n",
    "    data_for_drop = data_for_drop.dropna()\n",
    "    data_for_drop = shuffle(data_for_drop)\n",
    "    return data_for_drop\n",
    "\n",
    "def get_max_unbiased_dataset():\n",
    "    return dataset_manager.get_processed_dataset()\n",
    "\n",
    "data_imputed = make_imputed_dataset(data)\n",
    "data_dropped = get_dropped_dataset(data)\n",
    "data_max_unbiased = get_max_unbiased_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will divide the data into dataset and target variables and then I create predictor class instances and I will use cross-validation with 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.car_price_prediction.utils import df_utils\n",
    "\n",
    "def get_X_y(data):\n",
    "    X,y = df_utils.get_data_and_target(data)\n",
    "    X = pd.get_dummies(X)\n",
    "    return X,y\n",
    "\n",
    "X_imp, y_imp = get_X_y(data_imputed)\n",
    "X_drop, y_drop = get_X_y(data_dropped)\n",
    "X_unbias,y_unbias = get_X_y(data_max_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cv argument means how many folds I want to use in cross-validation, more folds mean more unbiased error, for forest I use 5, because it's computationally more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_score(estimator, est_name, cv,scoring = \"neg_mean_absolute_error\"):\n",
    "    score_imp = cross_val_score(estimator = estimator,X = X_imp,y = y_imp,cv = cv,scoring = scoring) \n",
    "    score_drop = cross_val_score(estimator=estimator,X = X_drop,y = y_drop,cv = cv,scoring = scoring) \n",
    "    score_unbias = cross_val_score(estimator=estimator, X = X_unbias, y = y_unbias, cv = cv, scoring = scoring)\n",
    "    print(\"MSE for imputed data for %s: \\t%.3f\" % (est_name,score_imp.mean()))\n",
    "    print(\"MSE for dropped data for %s: \\t%.3f\" % (est_name,score_drop.mean()))\n",
    "    print(\"MSE for unbiased data for %s: \\t%.3f\" % (est_name,score_unbias.mean()))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see how Linear Regression manages to predict on given datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for imputed data for Linear Regression: \t-6714.491\n",
      "MSE for dropped data for Linear Regression: \t-7794.199\n",
      "MSE for unbiased data for Linear Regression: \t-6647.573\n"
     ]
    }
   ],
   "source": [
    "print_cv_score(estimator = LinearRegression(),est_name = \"Linear Regression\",cv = 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use K-Nearest, with standartization, because without standartization, knn will not work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for imputed data for KNearestNeighbors: \t-5253.894\n",
      "MSE for dropped data for KNearestNeighbors: \t-5798.370\n",
      "MSE for unbiased data for KNearestNeighbors: \t-4540.195\n"
     ]
    }
   ],
   "source": [
    "scalar = StandardScaler()\n",
    "knn = KNeighborsRegressor()\n",
    "pipeline_knn = Pipeline([(\"sc\",scalar),('estimator',knn)])\n",
    "print_cv_score(pipeline_knn,est_name = \"KNearestNeighbors\",cv = 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last one I am going to use is Random Forest Regressor, it does not need any standartization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for imputed data for Random Forest: \t-2569.833\n",
      "MSE for dropped data for Random Forest: \t-2794.853\n",
      "MSE for unbiased data for Random Forest: \t-2247.920\n"
     ]
    }
   ],
   "source": [
    "scalar = StandardScaler()\n",
    "forest = RandomForestRegressor()\n",
    "print_cv_score(forest,est_name=\"Random Forest\",cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most biased dataset is dropped dataset, so we will not use it like our processed data, and also we can see that imputed dataset performs slightly worse than maximum unbiased dataset, it's due to the bias in imputed dataset, in next notebook, I will confirm, that the best dataset to use is indeed max_unbiased dateset, and I will prove it in completely new and unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
