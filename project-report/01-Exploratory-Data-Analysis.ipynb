{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Exploratory Data Analysis of car dataseet </h1></center>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will explore and analyze my data, visuliaze connections between features and do some statistic manipulations to show main characteristics of data. First I will import data, that \"page-scarper.py\" got from [cars.kg](cars.kg) and saved in \"cars_raw_data.csv\". I called raw, because later I will clean the data: fill missing values, detect and maybe eliminate outliers, and maybe rescale data(standartization, normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will import pandas, which is a library for data analysis, and you can see first 5 rows and last five raws of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\python36.zip',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\AsusInside\\\\.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6d8c9fd3828d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float_format'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar_price_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcar_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_raw_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('float_format', '{:.3f}'.format)\n",
    "from src.car_price_prediction.utils import dataset_manager\n",
    "\n",
    "car_data = dataset_manager.get_raw_dataset()\n",
    "car_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first thing I do, is check the shape of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can see the quantity of missing values in each of the columns. We can see that some of the features have a lot of missing values, and some not so much. I will go three ways, first I will create a model with a dataset where all the NaN values were dropped, and second I will impute all the missing NaN values and the third I will try to make maximum unbiased dataset, managing the data as a field expert. All of the three datasets will have different level of bias, I will explore it in 'Choosing-Dataset' notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I would like to see detailed statistical summary of data. From the summary, we can see that mean of data and standard deviation have extreme values, and looking at max values at dataset, I can suggest that maximum values skewed the mean and std methods, later I will eliminate or impute those extreme values in dataset, also in next summary we can see the datatype of each column that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I would want to see the correlation matrix between each of the continous(numeric) features.  \n",
    "By looking at summary above, I now know that dataset has extreme values in the data, each one of continious variables contains extreme values, which we need to eliminate, or make it NaN and impute, because if we won't, it will make a huge affect on correlations and predictions.  \n",
    "In the plot below we can see that correlation between continous variables is very low, it is due to the skewed data. There is no meaining at keeping analysis with this kind of data, at first I will eliminate extreme values just to see important relationships, I will not change dataset, the dataset will be changed and cleaned in \"Data-Cleaning\" notebook. I will clean data by \"cleaning-outliers.py\" script, by eliminating meaningless data, like: impossible mileage, capacity or engine power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_heatmap(data):\n",
    "    sns.set(style = \"white\")\n",
    "    corr = data.corr()\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "plot_heatmap(car_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset categorical features don't have extreme values, and continuous feature \"Год выпуска\" doesn't have extreme values too. I think it is due to the way people could post advertisements. Categorical features and year feature could be inputed via selection, and continous numbers could be of any range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will download data cleaned from outliers and impossible values, and we can see that number of NaN values increased, I made some extreme values NaN, so later i could impute that data. After, I will drop some non-needed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = dataset_manager.get_cleaned_outliers_dataset()\n",
    "car_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can continue our Exploritary Analysis. And right away we can see, that all statistic summary now makes full sense. And correlation plotting now is a little bit more reassuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(car_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to see, which features of dataset are important and which are not so. I am going to use RandomForest Regressor module from scklearn library to analyze importance of features. Also, I wll use OneHotEncoder to make categorical features numeric.  \n",
    "I will drop the NaN values for this example, because RandomForestRegressor doesn't take as an input data with missing values.\n",
    "I will replace NaN values in urgency column, cause there is a lot of missing values there.  \n",
    "And also I will drop power column, because of several reasons:\n",
    "* Power column is dependent from capacity column on 87.5 per cent, we can see it above in correlation matrix\n",
    "* Most of the people in KG doesn't care about the power, but do about capacity, that's why there are a lot of NaN values in power column, and a lot of impossible values\n",
    "* Later in notebooks, we will see that power column actually deteriorate prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from src.car_price_prediction.utils import df_utils\n",
    "\n",
    "car_data.dropna(inplace=True)\n",
    "car_data.drop(columns = [\"Мощность\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df_utils.get_data_and_target(car_data)\n",
    "X_dummies = pd.get_dummies(X)\n",
    "forest = RandomForestRegressor()\n",
    "feat_labels = X_dummies.columns[:-1]\n",
    "forest.fit(X_dummies,y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), forest.feature_importances_), X_dummies.columns.values), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make use of dimensionality reduction algorithm which is called Principal Component Analysis(PCA). It will allow us to reduce the number of dimensions of a dataset. To visualize data I will reduce the number of dimensions to two. At first we will scale the whole dataset so it will have near to equal sizes. We will scale dataset via StandartScaler module of scikit learn module.From this plot we can see the distribution of data, and outliers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = df_utils.scale_train(X)\n",
    "X_pca = pd.get_dummies(X_scaled)\n",
    "X_pca = pca.fit_transform(X_pca)\n",
    "plt.scatter(X_pca,y,marker = \"o\",lw=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.rank().corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished the exlporitary analysis of our data. Next thing I will do is a cleaning the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
